---
layout: tactic

title:  "Decrease model complexity"
tags: machine-learning algorithms design-tactic
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: algorithm-design
t-description: "Complex AI models have shown to have high energy consumption and therefore scaling down the model complexity can contribute to environmental sustainability. For example, using simple three-layered Convolutional Neural Network architecture to learn post-processing tasks of CT-scans  (Morotti et al),  using shallower Decision trees (Abreu et al 2020). "
t-participant: "Data Scientist"
t-artifact: "Algorithm"
t-context: "Machine Learning"
t-feature: "Inference"
t-intent: "Decreasing the model complexity makes ML algorithms simpler without sacrificing too much accuracy. These simplified models require less computing power which makes them more energy-efficient."
t-targetQA: "Energy efficiency"
t-relatedQA: 
t-measuredimpact: 
t-source: "Master Thesis 'Green tactics for ML-important QAs' by Heli Järvenpää (2023); 
Morotti, E., Evangelista, D., & Loli Piccolomini, E. (2021). A green prospective for learned post-processing in sparse-view tomographic reconstruction. Journal of Imaging, 7(8), 139.

Abreu, B. A., Grellert, M., & Bampi, S. (2020, October). Vlsi design of tree-based inference for low-power learning applications. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS) (pp. 1-5). IEEE."

t-source-doi: "DOI:10.3390/jimaging7080139;

DOI:10.1109/ISCAS45731.2020.9180704
"
t-diagram: "decrease-model-complexity.png"
---