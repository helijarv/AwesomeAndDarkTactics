---
layout: tactic

title:  "Use quantization-aware training"
tags: machine-learning model-training design-tactic
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: model-training
t-description: "Quantization-aware training is a technique used to train neural networks with the objective of achieving better performance and efficiency when using reduced precision data types, such as fixed-point or integer representations, instead of the more commonly used higher precision floating-point representations."
t-participant: "Data Scientist"
t-artifact: "Algorithm"
t-context: "Machine Learning"
t-feature: "Model Training"
t-intent: "Use quantization-aware training to convert high-precision data typed lo lower precision"
t-targetQA: "Accuracy"
t-relatedQA: "Energy-efficiency"
t-measuredimpact: 
t-source: "Master Thesis 'Green tactics for ML-important QAs' by Heli Järvenpää (2023); 
Kim, M., Saad, W., Mozaffari, M., & Debbah, M. (2022, May). On the tradeoff between energy, precision, and accuracy in federated quantized neural networks. In ICC 2022-IEEE International Conference on Communications (pp. 2194-2199). IEEE."
t-source-doi: "DOI:10.1109/ICC45855.2022.9838362"
t-diagram: "use-quantization-aware-training.png"
---