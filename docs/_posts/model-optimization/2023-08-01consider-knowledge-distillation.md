---
layout: tactic

title:  "Consider Knowledge Distillation"
tags: machine-learning model-optimization design-tactic
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: model-optimization
t-description: "Knowledge distillation is a technique where a large, complex model is used to train a smaller, simpler model. The goal is to transfer the learned information from the teacher model to the student model, allowing the student model to achieve comparable performance while requiring fewer computational resources."
t-participant: "Data Scientist"
t-artifact: "ML algorithm"
t-context: "Machine Learning"
t-feature: "Knowledge distillation"
t-intent: "If pre-trained models are too big for a given task, apply knowledge distillation of pre-trained models"
t-targetQA: "Performance"
t-relatedQA: "Accuracy, Energy efficiency"
t-measuredimpact: "Knowledge distallation improve performance when evaluating top 5 accuracy and energy-consumption"
t-source: "Master Thesis 'Green tactics for ML-important QAs ' by Heli Järvenpää (2023);

Shanbhag, S., Chimalakonda, S., Sharma, V. S., & Kaulgud, V. (2022, June). Towards a Catalog of Energy Patterns in Deep Learning Development. In Proceedings of the International Conference on Evaluation and Assessment in Software Engineering 2022 (pp. 150-159)."
t-source-doi: "DOI:10.1145/3530019.3530035"
t-diagram: "consider-knowledge-distillation.png"
---