---
layout: tactic

title:  "Enhance model sparsity"
tags: machine-learning model-optimization design-tactic energy-footprint
t-sort: "Awesome Tactic"
t-type: "Architectural Tactic"
categories: model-optimization
t-description: "Enhancing sparsity of a ML model means reducing the number of model parameters or setting their values to zero. For example, weight sparsification involves identifying and removing unnecessary or less important weights in a neural network. Enhancing model sparsity decreases the complexity of the model and consequently reduces requirements for storage and memory. Therefore it also results in lower power consumption"
t-participant: "Data Scientist"
t-artifact: "ML algorithm"
t-context: "Machine Learning"
t-feature: 
t-intent: "Remove unnecessary or less important weights in neural networks"
t-targetQA: "Energy-efficiency"
t-relatedQA: "Accuracy"
t-measuredimpact: "Lower energy consumption"
t-source: "Master Thesis 'Green tactics for ML-important QAs ' by Heli Järvenpää (2023);

Yang, X., Hua, S., Shi, Y., Wang, H., Zhang, J., & Letaief, K. B. (2020). Sparse optimization for green edge AI inference. Journal of communications and information networks, 5(1), 1-15."
t-source-doi: 
t-diagram: "enhance-model-sparsity.png"
---